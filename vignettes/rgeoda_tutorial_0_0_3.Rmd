---
title: "Tutorial of rgeoda v0.0.3"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{rgeoda_tutorial_0_0_3}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

rgeoda is a R library for spatial data analysis. It is a R wrapper of the libgeoda C++ library, which is built based on the GeoDa software. The version used in this tutorial is version 0.0.3.


## 1. Install rgeoda

Like GeoDa desktop software, `rgeoda` are avaiable to different platforms including: Mac, Linux and Windows. 

### Mac OSX and Linux

In R console, use install.packages() function to install rgeoda from its source pacakge at: https://github.com/lixun910/rgeoda/archive/0.0.3.tar.gz

```r
install.packages("https://github.com/lixun910/rgeoda/archive/0.0.3.tar.gz")
# or the development version
# devtools::install_github("lixun910/rgeoda")
```

### Windows

In R console, use install.packages() function to install rgeoda from its source pacakge at: https://github.com/lixun910/rgeoda/releases/download/0.0.3/rgeoda_0.0.3.zip

```r
install.packages("https://github.com/lixun910/rgeoda/releases/download/0.0.3/rgeoda_0.0.3.zip")
# or the development version
# devtools::install_github("lixun910/rgeoda")
```
Install rgeoda on windows from source package is not recommended. You would try if you know how to deal with [R devtools](https://www.r-project.org/nosvn/pandoc/devtools.html) on windows.

### Load rgeoda library in R

If everything installed without error, you should be able to load rgeoda:


```{r setup}
library(rgeoda)
```

## 2. Load Spatial Data

The data formats that rgeoda can read directly includes: ESRI Shapefile, MapInfo File, CSV, GML, GPX, KML, GeoJSON, TopoJSON, OpenFileGDB, GFT Google Fusion Tables, CouchDB 

Note: in this tutorial, we only tested loading ESRI shapefiles using rgeoda v0.0.3. Please create a ticket in rgeoda's [repository](https://github.com/lixun910/rgeoda/issues) if you experience any issues when loading spatial data. 

For example, to load the ESRI Shapefile `Guerry.shp` comes with the package:

```{r}
guerry_path <- system.file("extdata", "Guerry.shp", package = "rgeoda")
guerry <- geoda_open(guerry_path)
```

The `geoda_open` function returns a geoda object, which can be used to access the meta-data, fields, and columns of the input dataset.

#### 2.1 Attributes of `geoda` object

* n_cols
* n_obs
* field_names
* field_types
* table (data.frame)
* GetUndefinesCol(col_name)

```{r}
cat("\nnumber of columns:", guerry$n_cols)
cat("\nnumber of observations:", guerry$n_obs)
cat("\nfield names:", guerry$field_names)
cat("\nfield types:", guerry$field_types)
```

### 2.2 Access Table Data

The `geoda` instance has a data.frame attribute `table`, which stores the data loaded from the dataset. To get the values of "Crm_prp" column:

```{r}
crm_prp <- guerry$table$Crm_prp
cat("\nfirs 10 values of Crm_prp:", crm_prp[1:10])
```

## 3. Spatial Weights

Spatial weights are central components in spatial data analysis. The spatial weights represents the possible spatial interaction between observations in space. Like GeoDa desktop software, `rgeoda` provides a rich variety of methods to create several different types of spatial weights:

* Contiguity Based Weights: `queen_weights()`, `rook_weights()`
* Distance Based Weights: `distance_weights()`
* K-Nearest Neighbor Weights: `knn_weights()`
* Kernel Weights: `distance_weights()` and `knn_weights()` with kernel parameters

### 3.1 Queen Contiguity Weights

To create a Queen contiguity weights, we can call pygeoda's function 
```r
gda_queen(gda, order=1, include_lower_order = False, precision_threshold = 0)
``` 
by passing the GeoDa object `guerry` we just created: 

```{r}
queen_w <- queen_weights(guerry)
queen_w
```

The function `queen_weights()` returns an instance of 
`Weight` object. One can access the meta data of the spatial
weights by accessing the attributes of `GeoDaWeight` object:

#### Attributes of `Weight` object

* weight_type
* is_symmetric
* sparsity
* density
* min_nbrs
* max_nbrs
* mean_nbrs
* median_nbrs
* bool HasIsolates()
* [] GetNeighbors(idx)
* double SpatialLag(idx, [data])
* SaveToFile()

We can also access the details of the weights: e.g. list the neighbors of a specified observation, which is very helpful in exploratory spatial data analysis (which is focused in next tutorial):

```{r}
nbrs <- queen_w$GetNeighbors(0)
cat("\nNeighbors of 0-st observation are:", nbrs)
```
We can also compute the spatial lag of a specified observation by passing the values of the selected variable:

```{r}
lag0 <- queen_w$SpatialLag(0, crm_prp)
cat("\nSpatial lag of 0-st observation is:", lag0)
```

### 3.2 Rook Contiguity Weights

To create a Rook contiguity weights, we can call pygeoda's function 
```r
rook_weights(gda, order=1,include_lower_order=False, precision_threshold = 0)
``` 
by passing the GeoDa object `guerry` we just created: 

```{r}
rook_w <- rook_weights(guerry)
rook_w
```

The weights we created are in memory, which makes it straight forward for spatial data analysis and also are good for programming your application. To save the weights to a file, we need to call GeoDaWeight's function 
```r
SaveToFile(ofname, layer_name, id_name, id_vec)
```

The `layer_name` is the layer name of loaded dataset. For a ESRI shapefile, the layer name is the file name without the suffix (e.g. Guerry). 

The `id_name` is a key (column name), which means the associated column contains unique values, that makes sure that the weights are connected to the correct observations in the data table. 

The `id_vec` is the actual column data of `id_name`, it could be a tuple of integer or string values.

For example, in Guerry dataset, the column "CODE_DE" can be used as a key to save a weights file:

```{r}
#rook_w$SaveToFile('/Users/xun/Downloads/Guerry_r.gal', 'Guerry', 'CODE_DE', guerry$GetIntegerCol('CODE_DE'))
```

Then, we should find the file "Guerry_r.gal" in the output directory.

### 3.3 Distance Based Weights

To create a Distance based weights, we can call rgeoda's function 
```r
gda_distance_weights(gda, dist_thres, power=1.0,  is_inverse=False, is_arc=False, is_mile=True)
``` 
by passing the GeoDa object `guerry` we just created and the value of distance threshold. Like GeoDa, rgeoda provides a function to help you find a optimized distance threshold that guarantees that every observation has at least one neighbor:

```r
gda_min_distthreshold(GeoDa gda, bool is_arc = False, bool is_mile = True)
```

```{r}
dist_thres <- min_distthreshold(guerry)
dist_w <- distance_weights(guerry, dist_thres)
dist_w
```
### 3.4 K-Nearest Neighbor Weights

A special case of distance based weights is K-Nearest neighbor weights, in which every obersvation will have exactly k neighbors. To create a KNN weights, we can call pygeoda's function:
```r
gda_knn_weights(gda, k, power = 1.0,is_inverse = False, is_arc = False, is_mile = True)
```

For example, to create a 6-nearest neighbor weights using Guerry dataset:
```{r}
knn6_w <- knn_weights(guerry, 6)
knn6_w
```

### 3.5 Kernel Weights

Kernel weights apply kernel function to determine the distance decay in the derived continuous weights kernel. The kernel weights are defined as a function K(z) of the ratio between the distance dij from i to j, and the bandwidth hi, with z=dij/hi. 

The kernl functions include

* triangular
* uniform 
* quadratic
* epanechnikov
* quartic
* gaussian

Two functions are provided in rgeoda to create kernel weights.

** Kernel Weights with adaptive bandwidth**

To create a kernel weights with fixed bandwith:
```{r}
bandwidth <- min_distthreshold(guerry)
kernel_w <- kernel_weights(guerry, bandwidth, "uniform")
kernel_w
```

Besides the options `is_inverse`, `power`, `is_arc` and `is_mile` that are the same with the distance based weights, this kernel weights function has another option:
```
use_kernel_diagonals	
(optional) FALSE (default) or TRUE, apply kernel on the diagonal of weights matrix
```

** Kernel Weights with adaptive bandwidth**

To create a kernel weights with adaptive bandwidth or using max Knn distance as bandwidth:
```{r}
adptkernel_w = kernel_knn_weights(guerry, 6, "uniform")

adptkernel_w
```

This kernel weights function two more options:
```
adaptive_bandwidth	
(optional) TRUE (default) or FALSE: TRUE use adaptive bandwidth calculated using distance of k-nearest neithbors, FALSE use max distance of all observation to their k-nearest neighbors

use_kernel_diagonals	
(optional) FALSE (default) or TRUE, apply kernel on the diagonal of weights matrix
```

## 4 Spatial Data Analysis


### 4.1 Local Spatial Autocorrelation

rgeoda 0.0.3 provids following methods for univariate local spatial autocorrelation statistics:


* Local Moran: local_moran()
* Local Geary: local_geary()
* Local Getis-Ord statistics: local_g() and local_gstar()
* Local Join Count: local_joincount()


Methods for bivariate and multivariate local spatial autocorrelation statistics, as well as global spatial autocorrelation satatistics, will be included in next release of rgeoda.

In this tutorial, we will only introduce how to call these methods using pygeoda. For more information about the local spatial autocorrelation statisticis, please read: http://geodacenter.github.io/workbook/6a_local_auto/lab6a.html. 

#### 4.1.1 Local Moran

The Local Moran statistic is a method to identify local clusters and local spatial outliers. For example, we can call  function `local_moran()` with the created Queen weights and the data “crm_prp” as input parameters:

```{r}
lisa <- local_moran(queen_w, crm_prp)
```

The `local_moran()` function will return a `lisa` object, which we can call its public functions to access the results of lisa computation.

For example, we can call the function `GetLISAValues()` to get the values of local Moran:

```{r}
lms <- lisa$GetLISAValues()
lms
```

To get the pseudo-p values of significance of local Moran computation:
```{r}
pvals <- lisa$GetPValues()
pvals
```

To get the cluster indicators of local Moran computation:

```{r}
cats <- lisa$GetClusterIndicators()
cats
```

The predefined values of the indicators of LISA cluster are:
```
0 Not significant
1 High-High
2 Low-Low
3 High-Low
4 Low-High
5 Neighborless
6 Undefined
```
which can be accessed via function `GetLabels()`:
```{r}
lbls <- lisa$GetLabels()
lbls
```

By default, the `local_moran()` function will run with some default parameters, e.g.:
```
permutation number: 999
seed for random number generator: 123456789
```
, which are identical to GeoDa desktop software so that we can replicate the results as using  GeoDa software. It is also easy to change the paremter and re-run the LISA computation by calling Run() function. 

For example, re-run the above local Moran example using 9999 permutations 
```{r}
lisa$SetPermutations(9999)
lisa$Run()
```
Then, we can use the same `lisa` object to get the new results after 9999 permutations:
```{r}
pvals <- lisa$GetPValues()
pvals
```

rgeoda uses GeoDa’s C++ code, in which multi-threading is used to accelerate the computation of LISA. We can specify how many threads to run the computation:

```{r}
lisa$SetThreads(4)
lisa$Run()
lisa$GetPValues()
```

#### 4.1.2 Local Geary

Local Geary is a type of LISA that focuses on squared differences/dissimilarity. A small value of the local geary statistics suggest positive spatial autocorrelation, whereas large values suggest negative spatial autocorrelation. 

For example, we can call the function `local_geary()` with the created Queen weights and the data “crm_prp” as input parameters:

```{r}
geary_crmprs <- local_geary(queen_w, crm_prp)
```

To get the cluster indicators of the local Geary computation:
```{r}
geary_crmprs$GetClusterIndicators()
```

To get the pseudo-p values of the local Geary computation:

```{r}
geary_crmprs$GetPValues()
```

#### 4.1.3 Multivariate Local Geary:

```{r}
data <- list(guerry$table$Crm_prs, guerry$table$Crm_prp, guerry$table$Litercy, guerry$table$Donatns, guerry$table$Infants, guerry$table$Suicids)

multigeary <- local_multigeary(queen_w, data)
```

To get the cluster indicators of the local Geary computation:
```{r}
multigeary$GetClusterIndicators()
```

#### 4.1.4 Local Getis-Ord Statistics

There are two types of local Getis-Ord statistics: one is computing a ratio of the weighted average of the values in the neighboring locations, not including the value at the location; while another type of statistic includes the value at the location in both numerator and denominator.

A value larger than the mean suggests a high-high cluster or hot spot, a value smaller than the mean indicates a low-low cluster or cold spot.

For example, we can call the function `local_g()` with the created Queen weights and the data “crm_prp” as input parameters:

```{r}
localg_crmprs <- local_g(queen_w, crm_prp)
```

To get the cluster indicators of the local G computation:

```{r}
localg_crmprs$GetClusterIndicators()
```

To get the pseudo-p values of the local G computation:
```{r}
localg_crmprs$GetPValues()
```

For the second type of local Getis-Ord statistics, we can call the function `local_gstar()` with the created Queen weights and the data “crm_prp” as input parameters:
```{r}
localgstar_crmprs <- local_gstar(queen_w, crm_prp)
```

#### 4.1.5 Local Join Count


Local Join Count is a method to identify local clusters for binary data by using a local version of the so-called BB join count statistic. The statistic is only meaningful for those observations with value 1. 

For example, we can load the columbus dataset, and call the function `local_joincount()` with a Queen weights and the data “nsa”, which is a set of binary (0,1) values, as input parameters:

```{r}
columbus_path <- system.file("extdata", "columbus.shp", package = "rgeoda")
columbus <- geoda_open(columbus_path)

columbus_w <- queen_weights(columbus)
localjc_crmprs <- local_joincount(columbus_w, columbus$table$nsa)
```
To get the cluster indicators of the local Join Count computation:

```{r}
localjc_crmprs$GetClusterIndicators()
```

To get the pseudo-p values of the local Join Count  computation:
```{r}
localjc_crmprs$GetPValues()
```

To get the number of neighbors of the local Join Count computation:
```{r}
localjc_crmprs$GetNumNeighbors()
```

#### 4.1.6 Multivariate Local Join Count:

```{r}
multijc <- local_multijoincount(queen_w, data)
```

To get the cluster indicators of the multivariate local join count computation:
```{r}
multijc$GetClusterIndicators()
```

#### 4.1.7 Quantile LISA

```{r}
qsa <- local_quantileSA(queen_w, 4, crm_prp)
```

To get the cluster indicators of the quantile LISA computation:
```{r}
qsa$GetClusterIndicators()
```

### 4.2 Spatializing Multivariate Analysis

#### 4.2.1 Pincinple Components

```{r}
#data <- list(guerry$table$Crm_prs, guerry$table$Crm_prp, guerry$table$Litercy, guerry$table$Donatns, guerry$table$Infants, guerry$table$Suicids)
std_data <- standardize_z(data)
pc <- pca(std_data)
summary(pc)
```

Get first 3 components
```{r}
get_kcomponents(pc, 3)
```

#### 4.2.1 Multi Dimensional Scaling
```{r}
#data <- list(guerry$table$Crm_prs, guerry$table$Crm_prp, guerry$table$Litercy, guerry$table$Donatns, guerry$table$Infants, guerry$table$Suicids)

mds_v <- mds(data, 2)
mds_v
```

### 4.3 Spatial Clustering

 
Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous. 

In GeoDa, there are three different approaches explicitly incorporate the contiguity constraint in the optimization process: SKATER, Redcap and Max-p. More more details, please check: http://geodacenter.github.io/workbook/8_spatial_clusters/lab8.html All of these methods are included in rgeoda 0.0.3.

For example, to apply spatial clustering on the Guerry dataset, we use the queen weights to define the spatial contiguity and select 6 variables for similarity measure: "Crm_prs", "Crm_prp", "Litercy", "Donatns", "Infants", "Suicids". 

The following code is used to get a 2D data vector for the selected variables:

```{r}
data <- list(guerry$table$Crm_prs, guerry$table$Crm_prp, guerry$table$Litercy, guerry$table$Donatns, guerry$table$Infants, guerry$table$Suicids)
```

#### 4.3.1 SKATER

The Spatial C(K)luster Analysis by Tree Edge Removal(SKATER) algorithm introduced by Assuncao et al. (2006) is based on the optimal pruning of a minimum spanning tree that reflects the contiguity structure among the observations. It provides an optimized algorithm to prune to tree into several clusters that their values of selected variables are as similar as possible.

The rgeoda's SKATER function is: 
```r
skater(k, w, data, distance_method='euclidean', bound_vals = [],  min_bound = 0, random_seed=123456789)
```

For example, to create 4 spatially contiguous clusters using Guerry dataset, the queen weights and the values of the 6 selected variables:
```{r}
guerry_clusters <- skater(4, queen_w, data)
```

This `skater()` function returns a 2D list, which represents 4 clusters. Each cluster is composed by several contiguity areas, e.g. 15, 74, 16, 55, 60, 39, 68, 33, 17, 82, 81, 0, 2, 40, 20, 80

rgeoda also provides utility functions to compute some descriptive statistics of the clustering results, e.g. to compute the ratio of between to total sum of squares:


```{r}
betweenss <- between_sumofsquare(guerry_clusters, data)
totalss <- total_sumofsquare( data)
ratio <-  betweenss / totalss
cat("The ratio of between to total sum of square:", ratio)
```

#### 4.3.2 REDCAP

REDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning) is developed by D. Guo (2008). Like SKATER, REDCAP starts from building a spanning tree with 3 different ways (single-linkage, average-linkage, and the complete-linkage). The single-linkage way leads to build a minimum spanning tree. Then,REDCAP provides 2 different ways (first‐order and full-order constraining) to prune the tree to find clusters. The first-order approach with a minimum spanning tree is exactly the same with SKATER. In GeoDa and rgeoda, the following methods are provided:

* First-order and Single-linkage
* Full-order and Complete-linkage
* Full-order and Average-linkage
* Full-order and Single-linkage

For example, to find 4 clusters using the same dataset and weights as above using REDCAP with Full-order and Complete-linkage method:

```{r}
redcap_clusters <- redcap(4, queen_w, data, "fullorder-completelinkage")
redcap_clusters

betweenss <- between_sumofsquare(redcap_clusters, data)
totalss <- total_sumofsquare( data)
ratio <- betweenss / totalss
cat("The ratio of between to total sum of square:", ratio)
```

#### 4.3.3 Max-p

The so-called max-p regions model (outlined in Duque, Anselin, and Rey 2012) uses a different approach and considers the regionalization problem as an application of integer programming. In addition, the number of regions is determined endogenously.

The algorithm itself consists of a search process that starts with an initial feasible solution and iteratively improves upon it while maintaining contiguity among the elements of each cluster. Like Geoda, rgeoda provides three different heuristic algorithms to find an optimal solution for max-p:

* greedy
* Tabu Search
* Simulated Annealing

Unlike SKATER and REDCAP that one can specify the number of clusters as an input paramter, max-p doesn't allow to specify the number of clusters explicitly, but a constrained variable and the minimum bounding value that each cluster should reach that are used to find an optimized number of clusters.

For example, to use `greedy` algorithm in maxp function with the same dataset and weights as above to find optimal clusters using max-p:

First, we need to specify, for example, every cluster must have population >= 3236.67 thousands people:

```{r}
bound_vals <- guerry$table$Pop1831
min_bound <- 3236.67 # 10% of Pop1831
```

Then, we can call the max-p function with "greedy" algorith, the bound values and minimum bound value:

```{r}
maxp_clusters <- maxp(queen_w, data, bound_vals, min_bound, "greedy")

betweenss <- between_sumofsquare(maxp_clusters, data)
ratio <- betweenss / totalss
cat("The ratio of between to total sum of square:", ratio)
```

We can also specify using `tabu search` algorithm in maxp function with the parameter of tabu length:

```{r}
maxp_tabu_clusters <- maxp(queen_w, data, bound_vals, min_bound, "tabu", tabu_length=95)

betweenss <- between_sumofsquare(maxp_tabu_clusters, data)
ratio <- betweenss / totalss
cat("The ratio of between to total sum of square:", ratio)
```

To apply `simulated annealing` algorithm in maxp function with the parameter of cooling rate:

```{r}
maxp_sa_clusters <- maxp(queen_w, data, bound_vals, min_bound, "sa", cool_rate=0.75)

betweenss <- between_sumofsquare(maxp_sa_clusters, data)
ratio <- betweenss / totalss
cat("The ratio of between to total sum of square:", ratio)
```

We can also increase the number of iterations for local search process by specifying the parameter `initial` (default value is 99):

```{r}
maxp_clusters <- maxp(queen_w, data, bound_vals, min_bound, "greedy", initial=1000)

betweenss <- between_sumofsquare(maxp_clusters, data)
ratio <- betweenss / totalss
cat("Tratio of between to total sum of square:", ratio)
```
